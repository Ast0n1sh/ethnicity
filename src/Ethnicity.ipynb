{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Name: Ethnicity\n",
    "Developed By: Nishant Sharma\n",
    "### Project Summary:\n",
    "\n",
    "### Objective:\n",
    "Create a model to predict the nationality of a person from an uploaded image, along with predicting their emotion.\n",
    "\n",
    "### Specifications:\n",
    "\n",
    "* Indian Nationality: Predict age, dress color, and emotion.\n",
    "* United States Nationality: Predict age and emotion.\n",
    "* African Nationality: Predict emotion and dress color.\n",
    "* Other Nationalities: Predict nationality and emotion.\n",
    "\n",
    "### Constraints:\n",
    "The model should reject ages below 10 and above 60, working only for ages in between.\n",
    "\n",
    "This project aims to develop a comprehensive model for nationality and emotion prediction based on uploaded images, with specific features predicted based on the nationality identified.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Block 1: Load the Data\n",
    "This block of code will help in loading the images and their respective labels from the dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# Constants\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Define the ImageDataGenerator\n",
    "class ImageDataGenerator(Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE):\n",
    "        valid_indices = [i for i, label in enumerate(labels) if -1 not in label]\n",
    "        self.image_paths = [image_paths[i] for i in valid_indices]\n",
    "        self.labels = [labels[i] for i in valid_indices]\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.indices = np.arange(len(self.image_paths))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_image_paths = [self.image_paths[i] for i in batch_indices]\n",
    "        batch_labels = [self.labels[i] for i in batch_indices]\n",
    "        images = []\n",
    "        batch_labels_dicts = {'nationality': [], 'age': [], 'emotion': [], 'dress_color': []}\n",
    "        for idx, img_path in enumerate(batch_image_paths):\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, self.image_size)\n",
    "                img = img / 255.0\n",
    "                images.append(img)\n",
    "                label = batch_labels[idx]\n",
    "                batch_labels_dicts['nationality'].append(label[0])\n",
    "                batch_labels_dicts['age'].append(label[1])\n",
    "                batch_labels_dicts['emotion'].append(label[2])\n",
    "                batch_labels_dicts['dress_color'].append(label[3])\n",
    "        return np.array(images), {key: np.array(value) for key, value in batch_labels_dicts.items()}\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "# Function to load images from directory with subfolders representing categories\n",
    "def load_images_from_directory_with_subfolders(directory, label_template):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subfolder in os.listdir(directory):\n",
    "        subfolder_path = os.path.join(directory, subfolder)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            img_path = os.path.join(subfolder_path, filename)\n",
    "            if os.path.isfile(img_path):\n",
    "                image_paths.append(img_path)\n",
    "                labels.append(label_template[int(subfolder)])  # Convert subfolder name to int and use as index for label\n",
    "    return image_paths, labels\n",
    "\n",
    "# Function to load images from directory without specific subfolder logic\n",
    "def load_images_simple(directory, label):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        img_path = os.path.join(directory, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(label)\n",
    "    return image_paths, labels\n",
    "\n",
    "# Load and combine datasets\n",
    "base_path = r\"C:\\Users\\nisha\\OneDrive\\Documents\\Ethnicity Dataset\"\n",
    "all_image_paths = []\n",
    "all_labels = []\n",
    "\n",
    "# AffectNet\n",
    "affectnet_path = os.path.join(base_path, \"AffectNet\")\n",
    "for folder in [\"train\", \"val\", \"test\"]:\n",
    "    folder_path = os.path.join(affectnet_path, folder)\n",
    "    paths, labels = load_images_from_directory_with_subfolders(folder_path, [[i, -1, -1, -1] for i in range(8)])\n",
    "    all_image_paths.extend(paths)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "# Audience\n",
    "audience_path = os.path.join(base_path, \"Audience\")\n",
    "paths, labels = load_images_simple(audience_path, [0, -1, -1, -1])\n",
    "all_image_paths.extend(paths)\n",
    "all_labels.extend(labels)\n",
    "\n",
    "# DeepFashion\n",
    "deepfashion_path = os.path.join(base_path, \"DeepFashion\")\n",
    "for category in [\"Men\", \"Women\"]:\n",
    "    category_path = os.path.join(deepfashion_path, category)\n",
    "    for item in os.listdir(category_path):\n",
    "        item_path = os.path.join(category_path, item)\n",
    "        paths, labels = load_images_simple(item_path, [1, -1, -1, -1])\n",
    "        all_image_paths.extend(paths)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "# IMFD\n",
    "imfd_path = os.path.join(base_path, \"IMFD\")\n",
    "for folder in [\"Test\", \"Train\"]:\n",
    "    folder_path = os.path.join(imfd_path, folder)\n",
    "    paths, labels = load_images_simple(folder_path, [2, -1, -1, -1])\n",
    "    all_image_paths.extend(paths)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "# RAF-DB\n",
    "raf_db_path = os.path.join(base_path, \"RAF-DB\")\n",
    "for folder in [\"test\", \"train\"]:\n",
    "    folder_path = os.path.join(raf_db_path, folder)\n",
    "    paths, labels = load_images_from_directory_with_subfolders(folder_path, [[3, -1, -1, -1] for i in range(8)])\n",
    "    all_image_paths.extend(paths)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "# UTKFace\n",
    "utkface_path = os.path.join(base_path, \"UTKFace\")\n",
    "paths, labels = load_images_simple(utkface_path, [0, 25, 1, 2])\n",
    "all_image_paths.extend(paths)\n",
    "all_labels.extend(labels)\n",
    "\n",
    "# Create data generators\n",
    "train_image_paths, val_image_paths, train_labels, val_labels = train_test_split(all_image_paths, all_labels, test_size=0.2, random_state=42)\n",
    "train_generator = ImageDataGenerator(train_image_paths, train_labels)\n",
    "val_generator = ImageDataGenerator(val_image_paths, val_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Block 2: Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86528</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">44,302,848</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ nationality (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,052</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ age (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ emotion (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,052</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dress_color (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,052</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m,  │        \u001b[38;5;34m896\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m,  │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m,    │     \u001b[38;5;34m73,856\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86528\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_2[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │ \u001b[38;5;34m44,302,848\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ nationality (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m2,052\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ age (\u001b[38;5;33mDense\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m513\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ emotion (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m2,052\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dress_color (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m2,052\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,402,765</span> (169.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m44,402,765\u001b[0m (169.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,402,765</span> (169.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m44,402,765\u001b[0m (169.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "def create_model():\n",
    "    input_layer = Input(shape=(224, 224, 3))\n",
    "\n",
    "    # Convolutional layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Fully connected layers\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Output layers\n",
    "    nationality_output = Dense(4, activation='softmax', name='nationality')(x)  # Assuming 4 nationalities\n",
    "    age_output = Dense(1, activation='linear', name='age')(x)  # Regression for age\n",
    "    emotion_output = Dense(4, activation='softmax', name='emotion')(x)  # Assuming 4 emotions\n",
    "    dress_color_output = Dense(4, activation='softmax', name='dress_color')(x)  # Assuming 4 dress colors\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=[nationality_output, age_output, emotion_output, dress_color_output])\n",
    "\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss={\n",
    "                      'nationality': 'sparse_categorical_crossentropy',\n",
    "                      'age': 'mse',\n",
    "                      'emotion': 'sparse_categorical_crossentropy',\n",
    "                      'dress_color': 'sparse_categorical_crossentropy'\n",
    "                  },\n",
    "                  metrics={\n",
    "                      'nationality': 'accuracy', \n",
    "                      'age': 'mae', \n",
    "                      'emotion': 'accuracy', \n",
    "                      'dress_color': 'accuracy'\n",
    "                  })\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = create_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Block 3: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nisha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 415ms/step - age_mae: 2.8713 - dress_color_accuracy: 0.9643 - emotion_accuracy: 0.9844 - loss: 21.8489 - nationality_accuracy: 0.9730 - val_age_mae: 0.1827 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.0565 - val_nationality_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 408ms/step - age_mae: 1.5623 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 3.7899 - nationality_accuracy: 1.0000 - val_age_mae: 0.4536 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.2082 - val_nationality_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 422ms/step - age_mae: 1.5135 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 3.6498 - nationality_accuracy: 1.0000 - val_age_mae: 0.1381 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.0202 - val_nationality_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 425ms/step - age_mae: 1.5151 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 3.5831 - nationality_accuracy: 1.0000 - val_age_mae: 0.0597 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.0037 - val_nationality_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 424ms/step - age_mae: 1.4697 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 3.4095 - nationality_accuracy: 1.0000 - val_age_mae: 0.3004 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.0903 - val_nationality_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 423ms/step - age_mae: 1.4877 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 3.4786 - nationality_accuracy: 1.0000 - val_age_mae: 0.3692 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.1363 - val_nationality_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 387ms/step - age_mae: 1.4597 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 3.3459 - nationality_accuracy: 1.0000 - val_age_mae: 0.2828 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.0800 - val_nationality_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 388ms/step - age_mae: 1.4404 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 3.2635 - nationality_accuracy: 1.0000 - val_age_mae: 0.2016 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.0407 - val_nationality_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 389ms/step - age_mae: 1.4059 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 3.0818 - nationality_accuracy: 1.0000 - val_age_mae: 0.2151 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.0463 - val_nationality_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 390ms/step - age_mae: 1.3923 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 3.0110 - nationality_accuracy: 1.0000 - val_age_mae: 0.1940 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.0377 - val_nationality_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 390ms/step - age_mae: 1.3430 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 2.8455 - nationality_accuracy: 1.0000 - val_age_mae: 0.4525 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.2047 - val_nationality_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 389ms/step - age_mae: 1.3744 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 2.9740 - nationality_accuracy: 1.0000 - val_age_mae: 0.5384 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.2899 - val_nationality_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 396ms/step - age_mae: 1.3391 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 2.8080 - nationality_accuracy: 1.0000 - val_age_mae: 0.2064 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.0426 - val_nationality_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 402ms/step - age_mae: 1.3455 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 2.8292 - nationality_accuracy: 1.0000 - val_age_mae: 0.0068 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 4.6797e-05 - val_nationality_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 401ms/step - age_mae: 1.3179 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 2.7207 - nationality_accuracy: 1.0000 - val_age_mae: 0.1058 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.0112 - val_nationality_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 405ms/step - age_mae: 1.3017 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 2.6706 - nationality_accuracy: 1.0000 - val_age_mae: 0.2136 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.0456 - val_nationality_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 405ms/step - age_mae: 1.3087 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 2.7135 - nationality_accuracy: 1.0000 - val_age_mae: 0.3559 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.1267 - val_nationality_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 404ms/step - age_mae: 1.2889 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 2.6179 - nationality_accuracy: 1.0000 - val_age_mae: 0.2844 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.0809 - val_nationality_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 405ms/step - age_mae: 1.2795 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 2.5708 - nationality_accuracy: 1.0000 - val_age_mae: 0.8320 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.6923 - val_nationality_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 407ms/step - age_mae: 1.2969 - dress_color_accuracy: 1.0000 - emotion_accuracy: 1.0000 - loss: 2.8933 - nationality_accuracy: 1.0000 - val_age_mae: 0.5421 - val_dress_color_accuracy: 1.0000 - val_emotion_accuracy: 1.0000 - val_loss: 0.2939 - val_nationality_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Block 4: Evaluation and Loading Up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_generator):\n",
    "    results = model.evaluate(test_generator, verbose=1)\n",
    "    print(\"Test Loss, Test Accuracy:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nisha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 18 variables whereas the saved optimizer has 34 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# Save the model in the newer .keras format\n",
    "model.save('path_to_my_model.keras')\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('path_to_my_model.keras')\n",
    "\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "\n",
    "model = tf.keras.models.load_model(\n",
    "    'path_to_my_model.keras',\n",
    "    custom_objects={'mse': MeanSquaredError}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Block 5: Building a GUI for Emotion Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your trained model\n",
    "model = load_model('path_to_my_model.keras')\n",
    "\n",
    "def load_image():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        # Load and display the image\n",
    "        img = Image.open(file_path)\n",
    "        img.thumbnail((224, 224))  # Resize the image to fit in the display area\n",
    "        img_display = ImageTk.PhotoImage(img)\n",
    "        panel.configure(image=img_display)\n",
    "        panel.image = img_display\n",
    "\n",
    "        # Prepare the image for prediction\n",
    "        img = img.resize((224, 224))\n",
    "        img_array = np.array(img)\n",
    "        img_array = img_array / 255.0  # Normalize\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Predict using the model\n",
    "        predictions = model.predict(img_array)\n",
    "        results_text = f\"Nationality: {np.argmax(predictions[0])}\\n\" \\\n",
    "                       f\"Age: {predictions[1][0][0]:.2f}\\n\" \\\n",
    "                       f\"Emotion: {np.argmax(predictions[2])}\\n\" \\\n",
    "                       f\"Dress Color: {np.argmax(predictions[3])}\"\n",
    "        results_label.config(text=results_text)\n",
    "\n",
    "# Set up the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Image Prediction\")\n",
    "\n",
    "# Create a panel to display the image\n",
    "panel = tk.Label(root)\n",
    "panel.pack()\n",
    "\n",
    "# Create a button to load the image\n",
    "load_button = tk.Button(root, text=\"Load Image\", command=load_image)\n",
    "load_button.pack()\n",
    "\n",
    "# Create a label to display the predictions\n",
    "results_label = tk.Label(root, text=\"\", pady=20)\n",
    "results_label.pack()\n",
    "\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
